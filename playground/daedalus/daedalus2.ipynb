{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9owAi2LSQ53-"
      },
      "outputs": [],
      "source": [
        "# update gdown, used to download stuff from google drive\n",
        "!pip install -q --upgrade gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GBdT44axylCA"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "!gdown -q -O dataset.zip 1Mrx0OKnBFteOw1q8IZy-n8x9q8cxZwhT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3fPEx1_xys7w"
      },
      "outputs": [],
      "source": [
        "# unzip dataset\n",
        "!unzip -q -o dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fH2dc36xtGg",
        "outputId": "44f2432a-884e-418f-87bf-739a416349af"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import tensorflow as tf\n",
        "\n",
        "from loguru import logger\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "DAEDALUS2_DIR = pathlib.Path(\"/workspaces/playground/playground/daedalus\")\n",
        "\n",
        "TRAIN_DATASET_DIR = DAEDALUS2_DIR / \"post-processed\"\n",
        "IMAGE_SHAPE = (112, 112)\n",
        "CLASS_COUNT = 2996\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "DATABASE_DIR = DAEDALUS2_DIR / \"features_database\"\n",
        "MODEL_WEIGHTS_PATH = DAEDALUS2_DIR / \"feature_extractor\" / \"weights\"\n",
        "\n",
        "RNG_SEED = 42\n",
        "\n",
        "# ensure directories exist\n",
        "assert TRAIN_DATASET_DIR.exists()\n",
        "MODEL_WEIGHTS_PATH.parent.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_if_images_have_same_shape(\n",
        "    dataset_dir: pathlib.Path = TRAIN_DATASET_DIR,\n",
        ") -> None:\n",
        "    paths = dataset_dir.rglob(\"*.jpg\")\n",
        "    imgs = [cv2.imread(str(p)) for p in paths]\n",
        "    shapes = [img.shape for img in imgs]\n",
        "    return np.all(np.asarray(shapes)), shapes[0]\n",
        "\n",
        "\n",
        "# check_if_images_have_same_shape()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "909BnN-3yZmo",
        "outputId": "6a8342f5-5e9b-4afb-f6cb-71a6d623de36"
      },
      "outputs": [],
      "source": [
        "def load_dataset(\n",
        "    dataset_dir: pathlib.Path = TRAIN_DATASET_DIR,\n",
        "    rng_seed: int = RNG_SEED,\n",
        "    batch_size: int = BATCH_SIZE,\n",
        ") -> tf.data.Dataset:\n",
        "    ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        directory=dataset_dir,\n",
        "        batch_size=None,\n",
        "        image_size=IMAGE_SHAPE,\n",
        "        label_mode=\"categorical\",\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        ds.cache()\n",
        "        .shuffle(\n",
        "            buffer_size=ds.cardinality().numpy(),\n",
        "            seed=rng_seed,\n",
        "            reshuffle_each_iteration=True,\n",
        "        )\n",
        "        .batch(batch_size, drop_remainder=True)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "\n",
        "# load_dataset().element_spec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tLZfcdhdOrLg"
      },
      "outputs": [],
      "source": [
        "def create_models() -> tf.keras.Model:\n",
        "    base = tf.keras.applications.resnet.ResNet50(\n",
        "        weights=\"imagenet\",\n",
        "        input_shape=IMAGE_SHAPE + (3,),\n",
        "        include_top=False,\n",
        "    )\n",
        "\n",
        "    for layer in base.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # the arch is not particularly important\n",
        "    flatten = tf.keras.layers.Flatten()(base.output)\n",
        "    dense1 = tf.keras.layers.Dense(512, activation=\"relu\")(flatten)\n",
        "    dense1 = tf.keras.layers.BatchNormalization()(dense1)\n",
        "    dense2 = tf.keras.layers.Dense(256, activation=\"relu\")(dense1)\n",
        "    dense2 = tf.keras.layers.BatchNormalization()(dense2)\n",
        "    output = tf.keras.layers.Dense(256)(dense2)\n",
        "\n",
        "    feature_extractor = tf.keras.Model(\n",
        "        inputs=base.input,\n",
        "        outputs=output,\n",
        "        name=\"feature_extractor\",\n",
        "    )\n",
        "\n",
        "    softmax = tf.keras.layers.Dense(CLASS_COUNT, \"softmax\")(output)\n",
        "    classifier = tf.keras.Model(\n",
        "        inputs=base.input,\n",
        "        outputs=softmax,\n",
        "        name=\"classifier\",\n",
        "    )\n",
        "\n",
        "    classifier.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=\"accuracy\",\n",
        "    )\n",
        "\n",
        "    return feature_extractor, classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkIccjZjkaxA",
        "outputId": "09cfb1c4-55a4-48aa-b858-d52996db5f7a"
      },
      "outputs": [],
      "source": [
        "def load_or_create_feature_extractor(\n",
        "    train_dataset_dir: pathlib.Path = TRAIN_DATASET_DIR,\n",
        "    model_weights_path: pathlib.Path = MODEL_WEIGHTS_PATH,\n",
        ") -> tf.keras.Model:\n",
        "    feature_extractor, classifier = create_models()\n",
        "\n",
        "    try:\n",
        "        classifier.load_weights(model_weights_path).expect_partial()\n",
        "    except tf.errors.NotFoundError:\n",
        "        ds = load_dataset(dataset_dir=train_dataset_dir)\n",
        "        classifier.fit(ds, epochs=2)\n",
        "        classifier.save_weights(model_weights_path, save_format=\"tf\")\n",
        "\n",
        "    return feature_extractor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image_and_extract_features(\n",
        "    image_path: pathlib.Path,\n",
        "    feature_extractor: tf.keras.Model,\n",
        ") -> npt.NDArray[np.float32]:\n",
        "    img = cv2.imread(str(image_path))\n",
        "    batched_image = img.reshape(1, *img.shape)\n",
        "    return feature_extractor.predict(batched_image).flatten()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_new_instance_to_database(\n",
        "    image_path: pathlib.Path,\n",
        "    instance_label: str,\n",
        "    database_dir: pathlib.Path,\n",
        "    feature_extractor: tf.keras.Model,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    `instance_path`: path to the image to be added to the database\n",
        "    `instance_label`: the name or identifier of the the instance\n",
        "    `database_dir`: location of the database\n",
        "    `feature_extractor`: a pre-trained neural network that generated feature vectors\n",
        "    \"\"\"\n",
        "\n",
        "    logger.info(f\"storing new instance, label={instance_label}, path={image_path}\")\n",
        "\n",
        "    feature_vector = load_image_and_extract_features(\n",
        "        image_path=image_path,\n",
        "        feature_extractor=feature_extractor,\n",
        "    )\n",
        "\n",
        "    # create, if necessary, the label dir\n",
        "    label_dir = database_dir / instance_label\n",
        "    label_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # store the image on the database\n",
        "    stored_image_path = label_dir / image_path.name\n",
        "\n",
        "    if stored_image_path.exists():\n",
        "        logger.warning(\n",
        "            \"there is already an instance with this filename in the database, overwriting\"\n",
        "        )\n",
        "\n",
        "    stored_image_path.write_bytes(image_path.read_bytes())\n",
        "\n",
        "    # store feature vector on the database\n",
        "    feature_vector_path = stored_image_path.with_suffix(\".feature_vector\")\n",
        "    np.save(\n",
        "        file=feature_vector_path,\n",
        "        arr=feature_vector,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-09 14:43:36.003586: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-09 14:43:36.392251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46712 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:61:00.0, compute capability: 8.6\n",
            "2023-03-09 14:43:38.385 | INFO     | __main__:add_new_instance_to_database:14 - storing new instance, label=Vitali_Klitschko, path=/workspaces/playground/playground/daedalus/post-processed/Vitali_Klitschko/Vitali_Klitschko_0003_0001.jpg\n",
            "2023-03-09 14:43:39.694956: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101\n"
          ]
        }
      ],
      "source": [
        "def populate_database_with_train_dataset(\n",
        "    feature_extractor: tf.keras.Model,\n",
        "    train_dataset_dir: pathlib.Path = TRAIN_DATASET_DIR,\n",
        "    database_dir: pathlib.Path = DATABASE_DIR,\n",
        ") -> None:\n",
        "    for label_dir in train_dataset_dir.iterdir():\n",
        "        for image_path in label_dir.iterdir():\n",
        "            add_new_instance_to_database(\n",
        "                image_path=image_path,\n",
        "                instance_label=label_dir.name,\n",
        "                database_dir=database_dir,\n",
        "                feature_extractor=feature_extractor,\n",
        "            )\n",
        "\n",
        "populate_database_with_train_dataset(\n",
        "    feature_extractor=load_or_create_feature_extractor()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((12000, 256), (12000,))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_feature_vectors_and_labels(\n",
        "    database_dir: pathlib.Path = DATABASE_DIR,\n",
        ") -> KNeighborsClassifier:\n",
        "    label_dirs = sorted(database_dir.iterdir())\n",
        "    label_names = [path.name for path in label_dirs]\n",
        "    label_encoder = LabelEncoder().fit(label_names)\n",
        "\n",
        "    paths = list(database_dir.rglob(\"*.npy\"))\n",
        "\n",
        "    features = np.array([np.load(p) for p in paths])\n",
        "\n",
        "    labels = [p.parent.name for p in paths]\n",
        "    encoded_labels = label_encoder.transform(labels)\n",
        "    \n",
        "    return features, encoded_labels\n",
        "\n",
        "features, labels = load_feature_vectors_and_labels()\n",
        "features.shape, labels.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6051.14s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        }
      ],
      "source": [
        "# !gdown -q -O marquinho_treino.jpg 1EgvzTNEWTXvegURlmJAt8OXOtrKAlQEb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pathlib' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify_instance\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     instance_path: pathlib\u001b[39m.\u001b[39mPath,\n\u001b[1;32m      3\u001b[0m     feature_extractor: tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel,\n\u001b[1;32m      4\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m      5\u001b[0m     database_features, database_labels \u001b[39m=\u001b[39m load_feature_vectors_and_labels()\n\u001b[1;32m      6\u001b[0m     model \u001b[39m=\u001b[39m KNeighborsClassifier()\u001b[39m.\u001b[39mfit(database_features, database_labels)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pathlib' is not defined"
          ]
        }
      ],
      "source": [
        "def classify_instance(\n",
        "    instance_path: pathlib.Path,\n",
        "    feature_extractor: tf.keras.Model,\n",
        ") -> int:\n",
        "    database_features, database_labels = load_feature_vectors_and_labels()\n",
        "    model = KNeighborsClassifier().fit(database_features, database_labels)\n",
        "\n",
        "    img = cv2.imread(str(instance_path))\n",
        "    batched_image = img.reshape(1, *img.shape)\n",
        "    instance_features = feature_extractor.predict(batched_image).flatten()\n",
        "\n",
        "    return model.predict([instance_features])\n",
        "\n",
        "classify_instance(\n",
        "    pathlib.Path(\"/workspaces/playground/playground/daedalus/features_database/Aaron_Eckhart/Aaron_Eckhart_0001_0000.jpg\"),\n",
        "    feature_extractor=load_or_create_feature_extractor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
